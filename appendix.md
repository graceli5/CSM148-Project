# APPENDIX

## Exploratory Data Analysis

Our first important step in this project was conducting exploratory data analysis in order to refine our classification problem. We printed out the unique genre labels that were present as classes in the original dataset, and used this information to strategically sort related categories into five distinct genre groups. Then, we created a visualization of the distribution count of each category, in order to determine whether we had an even split of data. Once the data was prepared, we used a random split of 80% training and 20% testing data by calling scikit-learn’s train-test-split function. We enforced stratification as a parameter, which ensured that we had a representative divide of each genre category in both the training and testing data.
We also completed some exploratory data analysis that was not included in our final project, as it deviated from our driving goal. Namely, while we investigated a linear regression problem we created a heatmap correlation matrix that depicted the relationship strength between several numerical variables. This display helped us hone in on investigating the relationship between valence and danceability, as they had the highest correlation score at 0.57.

## Data Pre-Processing

To start our data cleaning, we identified and removed unnecessary columns like track_id and album_name that did not contribute to classification because they were extremely niche to each song. To ensure data quality, we first inspected the number of null values we had. Since we found only a handful which was a miniscule proportion of our dataset, we opted to drop all rows with missing values because this would not be losing significant data. Additionally, we wrote a line of code to ensure there were no duplicate entries based on track_name. For feature engineering, we normalized numerical features to ensure measurement metrics would weight each feature fairly. This was done using the StandardScaler package fitted on the training data, and transformed on the testing data in order to avoid data leakage.

Although the neural network model did not end up in our final code, in order to implement deep learning we also transformed the target variable genre into a numerical format through label encoding and one-hot-encoding. This allowed the neural network to perform a classification problem, despite its inability to work with string variables.

## Linear Regression

Because our correlation heatmap highlighted the relationship between valence and danceability, we decided to tackle a linear regression model between these two variables. We first implemented a classic linear regression model that fit on the training data, and we output the predicted regression line on a plot. In order to add regularization that could potentially prevent overfitting, we also attempted a lasso and a ridge regression model. To tune the regularization hyper-parameter for each of these, we used 10-fold cross validation to select the optimal alpha value, and then fit on our training set. These classic and regularized models were then scored on the testing data with metrics including rMSE, MAE, MAD, correlation, and R2, as these each tell us something slightly different about the accuracy. However, for all models the scoring metrics remained very low, for instance, rMSE and MAE were just around 0.2 for each.

Lasso regularization in particular is typically helpful in driving weight coefficients to zero as a feature selection technique, but in this case we were unable to make any significant feature selection conclusions due to the poor performance of the model and the fact that this task deviated from our main genre classification problem. Although these accuracy results were not strong, they did give us insights about the dataset. Particularly, we saw that the models were very underfit due to their low scores, which indicates that the linear model with two features is too simple. Instead, our dataset likely contains more complex patterns that involve relationships between many features, so we will look towards more multi-dimensional or non-linear models to make meaningful conclusions in the future.

## Logistic Regression

To implement logistic regression, we took our main goal of multi-class genre classification and simplified it to a binary classification problem of separating classical songs (labeled as 0) vs rock songs (labeled as 1). We also simplified the features we considered, and based on domain knowledge and their numerical nature decided on using only energy, speechiness, and loudness for prediction. The logistic regression model performed very well, achieving accuracy scores around 90% on training and testing splits. We analyzed this accuracy further by outputting a confusion matrix, and saw the predictions maintained a balance between false positives and false negatives, indicating no significant bias toward either genre. Further, using 5-fold cross-validation, we calculated AUC and output an ROC curve, which consistently yielded results in the 90% range. Although we were skeptical at first that these high results might indicate overfitting, the unseen validation data scored just as well which justified the significance of our results. The strong performance of the logistic model on both training and testing suggests that the chosen features were sufficient for accurate and generalizable classification, so regularization nor feature selection were not emphasized here.

The success of this exercise indicates that our data has well-separated clusters between these two genres, which provides a promising foundation for our multi-class genre problem. We also learned that there are strong patterns to be found when we combine multiple features, and that adding this complexity seems to construct far more successful predictions on this dataset than when we used a single variable in the linear regression case.

## KNN/Random Forest

We applied the KNN algorithm to the same binary classification problem above of distinguishing between classical and rock genres, using energy and loudness as predictive features. We knew the number of clusters hyper-parameter would be two, but for the number of neighbors hyper-parameter we performed 5-fold cross-validation on the training set with values between 3 and 19. The best-performing mean accuracy parameter value was 17, so we used this to fit our final KNN model.

The model achieved an accuracy of 88% on the training set. The confusion matrix showed a balanced distribution of false positives and false negatives, with the true positive rate (sensitivity) and true negative rate (specificity) both remaining high, and the F1 score reflected this strong performance. When tested on the validation set, the model achieved an AUC of 0.926 and cross-validated AUC scores between 0.88 and 1.0, with corresponding accuracy scores ranging from 78% to 92%. These results suggest the model generalized well to unseen data, similar to our above logistic regression result. Again, this model’s high accuracy indicates that our dataset has clear patterns between certain genres, as we had hoped.

However, because in the real-world there are far more genres than just two, we opted for a more challenging multi-class genre classification as our final task in order to tackle a more realistic problem. Random Forest was most successful on this four way genre-classification problem, and we describe the approach and reasoning behind this selection in detail in our main report.

## PCA

In addition to the KNN clustering described above, we also attempted PCA dimensionality reduction. We first ensured that our data features were all numerical and standardized, which allowed the calculations to occur smoothly. Then we applied singular value decomposition using numpy’s command on the input data, and examined the proportion of variability captured by each principal component using a scree plot. Unfortunately, PCA did not perform well for our dataset because the variance remained very evenly distributed across all principal components, with the cumulative variance not rising above 0.25 even after the first several. This indicated that no single component was able to capture a significant portion of the data's variance, reducing the effectiveness of dimensionality reduction and indicating a PCA transformed dataset might harm our model’s predictive accuracy. Since our dataset only had a couple dozen features in the first place, rather than hundreds or thousands, we decided it was reasonable to proceed without using PCA and instead use the full dataset for our models.

## Neural Network

An alternate attempt to our multi-genre classification problem was our implementation of a feedforward neural network. Because our categorical target variables could not be handled by the network, we applied label encoding and one-hot encoding to prepare the data for the model. We also created one more data split, so that now we had training, validation, and testing data. The model architecture we selected included two hidden layers with 64 and 32 neurons, both using the ReLU activation function, and the output layer which used a softmax activation function for multi-class classification. We also added a dropout layer with a 0.3 rate in order to prevent overfitting and act as an ensemble. In order to adjust the learning rate parameter and strike a balance between too small or too large gradient updates, the model was trained using the Adam optimizer and categorical cross-entropy loss function, with 25 epochs and a batch size of 40.

The final training accuracy reached 73.5%, with similar testing accuracy, indicating no significant overfitting. The training and validation loss curves showed consistent decreases, suggesting a good balance between learning and generalization. While the neural network's accuracy was slightly lower than the Random Forest model, it effectively captured non-linear relationships, so it provides a valuable alternative for genre classification. However, based on the slightly lower accuracy score and much higher model complexity of any neural network, we decided the random forest model would be a preferable final model choice because it is much less computationally expensive while presenting very similar results.

## Hyper-Parameter Tuning

In our project, hyperparameter tuning was very important to optimize model performance. For the Random Forest model, we tuned the number of estimators (iterating between choices 50, 100, 200) and maximum depth (10, 20, 30,100) using a grid search. The best combination was selected based on 5-fold cross-validation on the training set, and these optimal parameters resulted in a model with 77% accuracy on both training and testing data. For Lasso and Ridge regression, we tuned the regularization coefficient by again performing cross-validation on the training set, and selecting the alpha that minimized the cross-validated error which ended up being . Although these models did not outperform the Random Forest, the parameter-tuning ensured that the low performance we saw was a justified result rather than just poor model implementation. Additionally, for the KNN model, we optimized the number of neighbors for the model to consider (3 through 19) using 5-fold cross-validation. We plotted our training accuracy against this alpha parameter on a graph, and identified 17 as the optimal value. This tuning strategy resulted in a high accuracy and generalizable model that performed well on both training and testing sets. Lastly, during our neural network model it is worth noting that the learning rate, which is an extremely important hyper-parameter, was tuned automatically using the “Adam” optimizer on TensorFlow. This ensured that the model would use a proper step size to update its weights that avoids overshooting the minimum while still converging quickly.
